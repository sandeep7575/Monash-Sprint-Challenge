{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 3 in Assessment 2\n",
    "#### Student Name: Sandeep Kumar Kola\n",
    "#### Student ID: 28976657\n",
    "\n",
    "Date: 13/05/2018\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.6 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* pandas (for dataframes, included in Anaconda Python 3.6)\n",
    "* numpy (for arrays, included in Anaconda Python 3.6)\n",
    "* sklearn (for predicting missing values, needs to be downloaded)\n",
    "* Use pip install sklearn if the package is not installed on your machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Finding missing value and fill in the reasonable values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset3 = pd.read_csv(\"dataset3_with_missing.csv\")\n",
    "# Create copy of dataframe.\n",
    "dataset3_solution = dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3_solution.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at no of missing values:\n",
    "dataset3_solution.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Fix the sqft_living values\n",
    "* Upon observing the data we see that sqft_living is the sum of sqft_above and sqft_basement.\n",
    "* sqft_living = sqft_above + sqft_basement\n",
    "* From this relation let's fill the missing values for these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at few missing values of sqft_living\n",
    "dataset3_solution[np.isnan(dataset3_solution['sqft_living'])].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code imputes the missing values of sqft_living by adding the sqft_above and sqft_basement.\n",
    "dataset3_solution.loc[dataset3_solution['sqft_living'].isnull(), \n",
    "                      'sqft_living'] = dataset3_solution['sqft_above'] + dataset3_solution[\"sqft_basement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Fix the sqft_above values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code imputes the missing values of sqft_above by substracting the sqft_basement from sqft_living.\n",
    "dataset3_solution.loc[dataset3_solution['sqft_above'].isnull(), \n",
    "                      'sqft_above'] = dataset3_solution['sqft_living'] - dataset3_solution[\"sqft_basement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Fix the sqft_basement values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code imputes the missing values of sqft_basement by substracting the sqft_above from sqft_living.\n",
    "dataset3_solution.loc[dataset3_solution['sqft_basement'].isnull(), \n",
    "                      'sqft_basement'] = dataset3_solution['sqft_living'] - dataset3_solution[\"sqft_above\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if all the missing values are filled.\n",
    "dataset3_solution.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Missing Bedrooms values needs to be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Imputing the bathroom missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the mising values in bathrooms.\n",
    "bathrooms = dataset3_solution[\"bathrooms\"].unique()\n",
    "bathrooms = pd.DataFrame(bathrooms)\n",
    "bathrooms.columns = ['bathrooms']\n",
    "bathrooms = bathrooms.sort_values(by='bathrooms')\n",
    "bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the corelationmatrix for bathrooms.\n",
    "corr = dataset3_solution.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems like bathrooms are corelated to many other columns of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use Random forest algorithm to predict the missing bathroom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "final_test_data = dataset3_solution[np.isnan(dataset3['bathrooms'])]\n",
    "final_test_data = final_test_data.drop([\"bathrooms\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's process training data.\n",
    "train = dataset3_solution.dropna()\n",
    "# X is the dataset without bathroom values.\n",
    "X = train.drop([\"bathrooms\"],axis=1)\n",
    "# Dropping columns which are not useful in predciting bathrooms.\n",
    "# Drop the ID\n",
    "X = X.drop([\"id\"],axis=1)\n",
    "# Drop date and lat and lon values too.\n",
    "X = X.drop([\"date\", \"lat\", \"long\", \"zipcode\"],axis=1)\n",
    "# Y is the dataset with the bathroom values.\n",
    "y = train[\"bathrooms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the tran test split fucntion and split the data randomly at 75%.\n",
    "# This would be helpful in knowing the accuracy of the model.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at \n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's scale the values using standard scalar function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling train and test data.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# fit Fit_transform fucniton to the datasets.\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting Random Forest Classification to the training set.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Build classifier.\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "# Convert y_train to str since there are many categorical values in bathrooms.\n",
    "y_train = y_train.astype(str)\n",
    "# Fit the classifier to training data.\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's calculate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the predicted value dataset to dataframe.\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "y_pred.columns = ['pred']\n",
    "y_test.columns = ['Actual']\n",
    "\n",
    "# Convert to lists.\n",
    "y_pred_list = list(y_pred[\"pred\"])\n",
    "y_test_list = list(y_test[\"Actual\"])\n",
    "\n",
    "# Make actual dataframe.\n",
    "accuracy_df = pd.DataFrame(\n",
    "    {'Actual': y_test_list,\n",
    "     'Pred': y_pred_list })\n",
    "\n",
    "# Duplicated values are nothing but the correclty predicted values.\n",
    "dup_values = accuracy_df[accuracy_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dup_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuarcy of the model is: \n",
    "(2292/2392)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is having a good accuracy, let's predict the final test set missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing final train data.\n",
    "# It is important that the final data is also transformed with the same standard scaling and has same no of columns.\n",
    "# Drop the ID\n",
    "f_test_data = final_test_data.drop([\"id\"],axis=1)\n",
    "# drop date and lat and lon values too.\n",
    "f_test_data = f_test_data.drop([\"date\", \"lat\", \"long\", \"zipcode\"],axis=1)\n",
    "\n",
    "# Apply the same transform function.\n",
    "f_test_data = sc.transform(f_test_data)\n",
    "# Using the classifier predict function predict the missing values of bathrooms.\n",
    "f_y_pred = classifier.predict(f_test_data)\n",
    "\n",
    "# Make a dataframe of the predicted values.\n",
    "f_y_pred = pd.DataFrame(f_y_pred)\n",
    "f_y_pred.columns = ['pred']\n",
    "# Check the count of values.\n",
    "f_y_pred.pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the final dataframe.\n",
    "# Take id values into a list.\n",
    "id_list = list(final_test_data[\"id\"])\n",
    "# Take predicted values into a list.\n",
    "f_y_pred_list = list(f_y_pred[\"pred\"])\n",
    "# Create a temporary dataframe of the above two columns.\n",
    "temp = pd.DataFrame({\"id\" : id_list, \"bathrooms\" : f_y_pred_list })\n",
    "\n",
    "# Set id as index.\n",
    "temp.set_index('id', inplace=True)\n",
    "# Set id as index.\n",
    "dataset3_solution.set_index('id', inplace=True)\n",
    "\n",
    "# Replace the null values in the solution dataframe by the predicted values from the model.\n",
    "dataset3_solution.loc[dataset3_solution['bathrooms'].isnull(),\n",
    "                               'bathrooms'] = temp['bathrooms']\n",
    "\n",
    "# Check to see if any values are missing.\n",
    "dataset3_solution.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset the index of the dataframe.\n",
    "dataset3_solution = dataset3_solution.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3_solution.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the csv file.\n",
    "dataset3_solution.to_csv('dataset3_solution.csv', sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of Task 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
