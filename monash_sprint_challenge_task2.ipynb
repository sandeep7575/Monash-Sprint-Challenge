{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This script is a part of Monash Sprint Challenge'''\n",
    "   \n",
    "__author__ = 'Sandeep Kumar Kola'\n",
    "__email__ = 'sandeep.kola07@gmail.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 13/05/2018\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.6 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* pandas (for dataframes, included in Anaconda Python 3.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Integrating the Job datasets: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Read the data using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv(\"dataset1_solution.csv\")\n",
    "dataset2 = pd.read_csv(\"dataset2_integration.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check columns\n",
    "dataset1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some of the Column names are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use pandas concat to join the dataframes.\n",
    "dataset1_dataset2_solution = pd.concat([dataset1, dataset2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset1_dataset2_solution.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since column names are different we have data in two different columns for:\n",
    "* ContractType : Contract Type\n",
    "* ContractTime : Contract Time\n",
    "* SourceName : Source name\n",
    "* Location : location\n",
    "* SalaryPerAnnum : Salary per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Strategy is to impute the values of the dataset2 where found missing in dataset1_dataset2_solution and drop the extra columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute the missing values of \"ContractTime\" in the dataset1_dataset2_solution from the dataset2 values.\n",
    "dataset1_dataset2_solution.loc[dataset1_dataset2_solution['ContractTime'].isnull(),\n",
    "                               'ContractTime'] = dataset2['Contract Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute the missing values of \"ContractType\" in the dataset1_dataset2_solution from the dataset2 values.\n",
    "dataset1_dataset2_solution.loc[dataset1_dataset2_solution['ContractType'].isnull(),\n",
    "                               'ContractType'] = dataset2['Contract Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute the missing values of \"SourceName\" in the dataset1_dataset2_solution from the dataset2 values.\n",
    "dataset1_dataset2_solution.loc[dataset1_dataset2_solution['SourceName'].isnull(),\n",
    "                               'SourceName'] = dataset2['Source Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute the missing values of \"Location\" in the dataset1_dataset2_solution from the dataset2 values.\n",
    "dataset1_dataset2_solution.loc[dataset1_dataset2_solution['Location'].isnull(),\n",
    "                               'Location'] = dataset2['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace and change it to per annum by multiplying by 12\n",
    "dataset1_dataset2_solution.loc[dataset1_dataset2_solution['SalaryPerAnnum'].isnull(),\n",
    "                               'SalaryPerAnnum'] = dataset2['Salary per month']*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the unnecessary redundant columns.\n",
    "dataset1_dataset2_solution.drop(['Contract Type', 'Contract Time', 'Source Name', 'location', 'Salary per month'], \n",
    "                                axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at the data.\n",
    "dataset1_dataset2_solution.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Fix ContractType Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get unique values.\n",
    "dataset1_dataset2_solution[\"ContractType\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change it to values as per dataset1.\n",
    "dataset1_dataset2_solution.ContractType.replace({'n/a':'non-specified'},inplace=True)\n",
    "dataset1_dataset2_solution.ContractType.replace({'ft':'full-time'},inplace=True)\n",
    "dataset1_dataset2_solution.ContractType.replace({'pt':'part-time'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Fix ContractTime Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get unique values.\n",
    "dataset1_dataset2_solution[\"ContractTime\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change it to values as per dataset1.\n",
    "dataset1_dataset2_solution.ContractTime.replace({'n/a':'non-specified'},inplace=True)\n",
    "dataset1_dataset2_solution.ContractTime.replace({'contr.':'contract'},inplace=True)\n",
    "dataset1_dataset2_solution.ContractTime.replace({'perm.':'permanent'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Set Id as index and search for duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset1_dataset2_solution.set_index('Id', inplace=True)\n",
    "dataset1_dataset2_solution.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup_values = dataset1_dataset2_solution[dataset1_dataset2_solution.duplicated(keep='first')]\n",
    "dup_values.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dup_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 181 duplicated records. Keep the first unique record and delete the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset1_dataset2_solution = dataset1_dataset2_solution.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Global key.\n",
    "* Leaving Id aside, since Id is always a unique value let's find a global key which should identiy duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup_exist = dataset1_dataset2_solution[dataset1_dataset2_solution.duplicated(subset=['Category', 'CloseDate', 'Company', 'ContractTime', 'ContractType',\n",
    "       'Location', 'OpenDate'], keep='first')]\n",
    "dup_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets add one more column and check for globalkey.\n",
    "dup_exist = dataset1_dataset2_solution[dataset1_dataset2_solution.duplicated(subset=['Category', 'CloseDate', 'Company', 'ContractTime', 'ContractType',\n",
    "       'Location', 'OpenDate', 'SalaryPerAnnum'], keep='first')]\n",
    "dup_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaving Id, at a minimum we need to provide 'Category', 'CloseDate', 'Company', 'ContractTime', 'ContractType', 'Location', 'OpenDate' to identify duplicate rows. This is the global key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a csv file.\n",
    "dataset1_dataset2_solution.to_csv('dataset1_dataset2_solution.csv', sep=',')\n",
    "print(\"Check the working directory for the file dataset1_dataset2_solution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of the Task 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
